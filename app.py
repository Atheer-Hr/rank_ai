import streamlit as st
import pandas as pd
import numpy as np
import joblib
import tensorflow as tf
from tensorflow.keras.models import load_model
# Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¢Ù…Ù† Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø³Ø±ÙŠ:
import requests
import zipfile
from io import BytesIO 
import os 
import tempfile # Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„

# ======================================================================
# -------------------- 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„ Ø§Ù„Ø¢Ù…Ù† ÙˆØ§Ù„Ù…Ø¤Ù…Ù† --------------------
# ======================================================================

# ÙˆØ¸ÙŠÙØ© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ù…Ù„Ù Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø¤Ù‚Øª
def find_asset_path(base_path, filename):
    """ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙÙŠ Ø£ÙŠ Ù…Ø¬Ù„Ø¯ ÙØ±Ø¹ÙŠ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ."""
    for root, _, files in os.walk(base_path):
        if filename in files:
            # ÙŠØ¹ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù…Ù„Ù
            return os.path.join(root, filename)
    return None

# @st.cache_data: ÙŠØªÙ…ÙŠØ² Ø¨ÙƒÙˆÙ†Ù‡ Ø£ÙƒØ«Ø± Ø£Ù…Ø§Ù†Ù‹Ø§ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ù…Ù† @st.cache_resource
@st.cache_data(show_spinner="Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ù…Ø·Ø¨Ù‘Ø¹Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†...")
def load_assets_secure():
    # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£ÙŠ ÙØ´Ù„
    default_return = None, None, None, [], None, None, None, None

    if "ASSET_DOWNLOAD_URL" not in st.secrets:
        st.error("âš ï¸ ÙØ´Ù„: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ ASSET_DOWNLOAD_URL Ø§Ù„Ø³Ø±ÙŠ. Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")
        return default_return

    ASSET_URL = st.secrets["ASSET_DOWNLOAD_URL"]
    
    # --- STEP 1: Download Attempt and Status Check ---
    try:
        st.info(f"Ù…Ø­Ø§ÙˆÙ„Ø© ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„ Ù…Ù†: {ASSET_URL[:50]}...")
        response = requests.get(ASSET_URL, stream=True)
        
        if response.status_code != 200:
            st.error(f"âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„. Ø±Ù…Ø² Ø§Ù„Ø­Ø§Ù„Ø©: {response.status_code}. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø§Ø¨Ø· Ø®Ø·Ø£.")
            return default_return
            
        zip_content = response.content
        zip_size_mb = len(zip_content) / (1024 * 1024)
        st.info(f"âœ… ØªÙ… Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­. Ø§Ù„Ø­Ø¬Ù…: {zip_size_mb:.2f} Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª.")
        
    except Exception as e:
        st.error(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ (requests). ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ©/Ø§Ù„Ø±Ø§Ø¨Ø·. Ø§Ù„Ø®Ø·Ø£: {e}")
        return default_return

    # --- STEP 2: Extraction and Path Setup ---
    BASE_PATH = tempfile.mkdtemp() + "/"
    
    # ÙˆØ¸ÙŠÙØ© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ÙÙƒ Ø§Ù„Ø¶ØºØ· Ø§Ù„Ø¢Ù…Ù†
    def extract_assets_from_zip(zip_content, base_path):
        """ØªØ³ØªØ®Ù„Øµ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù€ ZIP Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø± Ù…Ø­Ø¯Ø¯."""
        try:
            with zipfile.ZipFile(BytesIO(zip_content)) as z:
                z.extractall(path=base_path)
            return True
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ÙÙƒ Ø§Ù„Ø¶ØºØ·: {e}")
            return False

    # 3. ÙÙƒ Ø§Ù„Ø¶ØºØ· (Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© Ù…Ø³Ø§Ø¹Ø¯Ø©)
    if not extract_assets_from_zip(zip_content, BASE_PATH):
        st.error("âŒ ÙØ´Ù„ ÙÙƒ Ø¶ØºØ· Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¶ØºÙˆØ·.")
        return default_return
    st.info(f"âœ… ØªÙ… ÙÙƒ Ø§Ù„Ø¶ØºØ· ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø¤Ù‚Øª: {BASE_PATH}")
        
    # --- STEP 3: Loading Assets with Path Validation ---
    try:
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø±Ù† (find_asset_path) Ù„ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø¤Ù‚Øª
        model_path = find_asset_path(BASE_PATH, 'ranking_model.h5')
        scaler_X_path = find_asset_path(BASE_PATH, 'scaler_X.pkl')
        scaler_y_path = find_asset_path(BASE_PATH, 'scaler_y.pkl')
        indicators_path = find_asset_path(BASE_PATH, 'indicator_names.txt')

        if not all([model_path, scaler_X_path, scaler_y_path, indicators_path]):
            st.error("âš ï¸ ÙØ´Ù„: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø­Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©. ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„Ù€ ZIP.")
            st.write(f"Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†Ù‡Ø§: [Model: {model_path}, Scaler_X: {scaler_X_path}, Indicators: {indicators_path}]")
            return default_return

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„
        st.info(f"Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Keras Ù…Ù†: {model_path}")
        model = load_model(model_path, compile=False)
        st.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­.")

        scaler_X = joblib.load(scaler_X_path)
        scaler_y = joblib.load(scaler_y_path)
        
        with open(indicators_path, 'r', encoding='utf-8') as f:
            indicator_names = [line.strip() for line in f]

        # 4. ØªØ¹Ø§Ø±ÙŠÙ Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ Ø§Ù„Ø«Ø§Ø¨ØªØ© (Ù„Ø¶Ù…Ø§Ù† Ø£Ù†Ù‡Ø§ Ù…ØªØ§Ø­Ø©)
        recommendations_map = {
            "Ø§Ù„ÙƒÙØ§Ø¡Ø© Ù„Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø¨Ø´Ø±ÙŠ": "ØªØ·ÙˆÙŠØ± Ø¨Ø±Ø§Ù…Ø¬ ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ù…Ø³ØªÙ…Ø±Ø© Ù„Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙØ±Ø¯ÙŠ.",
            "Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬": "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…Ù†Ø§Ù‡Ø¬ ÙˆØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ù„ØªØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø±Ù† 21.",
            "Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ": "Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³Ø§Ø±Ø§Øª Ù…Ù‡Ù†ÙŠØ© ÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ù…Ø¹ Ø­ÙˆØ§ÙØ² Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ø§Ø².",
            "ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø´Ø®ØµÙŠØ©": "Ø¥Ø·Ù„Ø§Ù‚ Ø¨Ø±Ø§Ù…Ø¬ Ø¥Ø±Ø´Ø§Ø¯ Ù†ÙØ³ÙŠ ÙˆØ§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù„Ù‚ÙŠØ§Ø¯Ø© Ù„Ø¯Ù‰ Ø§Ù„Ø·Ù„Ø§Ø¨.",
            "Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ØªØ±Ø¨ÙˆÙŠ": "ØªØ·ÙˆÙŠØ± Ø£Ø¯ÙˆØ§Øª ØªÙ‚ÙŠÙŠÙ… Ù…Ø¹ÙŠØ§Ø±ÙŠØ© Ø±Ù‚Ù…ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ Ù†ÙˆØ§ØªØ¬ Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ø¯Ù‚Ø©.",
            "Ø§Ù„Ø´Ø±Ø§ÙƒØ© Ù…Ø¹ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ": "ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø´Ø±Ø§ÙƒØ§Øª Ù…Ø¹ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ù„Ø¯Ø¹Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¹Ù…Ù„ÙŠ ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„ØªÙ‚Ù†ÙŠØ©.",
            "Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø§Ø³Ø±Ø©": "ØªÙØ¹ÙŠÙ„ Ù…Ø¬Ø§Ù„Ø³ Ø£ÙˆÙ„ÙŠØ§Ø¡ Ø§Ù„Ø£Ù…ÙˆØ± ÙˆØ¥Ø´Ø±Ø§ÙƒÙ‡Ù… ÙÙŠ Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ.",
            "Ø§Ù„Ù…Ø±Ø§ÙÙ‚ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØ§Ù„Ù…Ø¨Ø§Ù†ÙŠ": "ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© ÙˆØªÙˆÙÙŠØ± Ø¨ÙŠØ¦Ø© ØµÙÙŠØ© Ø¬Ø§Ø°Ø¨Ø© ÙˆØ¢Ù…Ù†Ø©.",
            "Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø§Ù„Ù…Ø¯Ø§Ø±Ø³": "ØªØ¹Ù…ÙŠÙ… Ø§Ù„ÙØµÙˆÙ„ Ø§Ù„Ø°ÙƒÙŠØ© ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨Ù…Ù†ØµØ§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø±Ù‚Ù…ÙŠØ©.",
            "Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ": "Ø¥Ø¯Ø®Ø§Ù„ Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø¯Ø§Ø¡ Ø±Ø¦ÙŠØ³ÙŠØ© (KPIs) Ù„Ù…ØªØ§Ø¨Ø¹Ø© ØªÙ‚Ø¯Ù… Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ.",
            "Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ³": "ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ¹Ù„Ù… Ù†Ø´Ø· ÙˆØªØ¯Ø±ÙŠØ³ ØªÙØ±ÙŠØ¯ÙŠØ© ØªØ±Ø§Ø¹ÙŠ Ø§Ù„ÙØ±ÙˆÙ‚ Ø§Ù„ÙØ±Ø¯ÙŠØ©.",
            "Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©": "Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ÙˆØ·Ù†ÙŠØ© Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ ÙˆØ§Ù„Ù…Ù†Ø§Ø·Ù‚."
        }
        
        execution_plan_map = {
            "Ø§Ù„ÙƒÙØ§Ø¡Ø© Ù„Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø¨Ø´Ø±ÙŠ": "ØªÙˆØ²ÙŠØ¹ Ø¨Ø±Ø§Ù…Ø¬ ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ø­Ø³Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø³Ù†ÙˆÙŠ.",
            "Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬": "ØªØ´ÙƒÙŠÙ„ Ù„Ø¬Ø§Ù† Ù…Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„Ù…Ù†Ø§Ù‡Ø¬ ÙˆØ±Ø¨Ø· Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø¨Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©.",
            "Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ": "ØªØµÙ…ÙŠÙ… Ù…Ø³Ø§Ø±Ø§Øª Ù…Ù‡Ù†ÙŠØ© ÙØ±Ø¯ÙŠØ© Ù…Ø¹ Ù…ØªØ§Ø¨Ø¹Ø© ÙØµÙ„ÙŠØ© ÙˆØªÙ‚ÙŠÙŠÙ… ØªØ·Ø¨ÙŠÙ‚ÙŠ.",
            "ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø´Ø®ØµÙŠØ©": "ØªÙ†ÙÙŠØ° Ø£Ù†Ø´Ø·Ø© ØµÙÙŠØ© ÙˆÙ„Ø§ØµÙÙŠØ© ØªØ¹Ø²Ø² Ø§Ù„Ù‚ÙŠØ§Ø¯Ø© ÙˆØ§Ù„Ø§Ù†Ø¶Ø¨Ø§Ø· Ø§Ù„Ø°Ø§ØªÙŠ.",
            "Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ØªØ±Ø¨ÙˆÙŠ": "Ø¥Ø¹Ø§Ø¯Ø© ØªØµÙ…ÙŠÙ… Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙ‚ÙˆÙŠÙ… ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ.",
            "Ø§Ù„Ø´Ø±Ø§ÙƒØ© Ù…Ø¹ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ": "ØªÙˆÙ‚ÙŠØ¹ Ø§ØªÙØ§Ù‚ÙŠØ§Øª ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø´Ø±ÙƒØ§Øª Ù…Ø­Ù„ÙŠØ© Ù„Ø¯Ø¹Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø±Ø§ÙÙ‚.",
            "Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø§Ø³Ø±Ø©": "Ø¥Ø·Ù„Ø§Ù‚ Ù…Ù†ØµØ© ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø£ÙˆÙ„ÙŠØ§Ø¡ Ø§Ù„Ø£Ù…ÙˆØ± ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡.",
            "Ø§Ù„Ù…Ø±Ø§ÙÙ‚ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØ§Ù„Ù…Ø¨Ø§Ù†ÙŠ": "ØªØ­Ø¯ÙŠØ¯ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„ØµÙŠØ§Ù†Ø© ÙˆØ§Ù„ØªØ¬Ù‡ÙŠØ² Ø­Ø³Ø¨ ÙƒØ«Ø§ÙØ© Ø§Ù„Ø·Ù„Ø§Ø¨.",
            "Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø§Ù„Ù…Ø¯Ø§Ø±Ø³": "ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨Ù…Ù†ØµØ§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ø¹Ù„ÙŠÙ‡Ø§.",
            "Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ": "ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø§Ù… Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø¯Ø§Ø¡ Ø´Ù‡Ø±ÙŠ ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„ØªØ­ÙÙŠØ² Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠ.",
            "Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ³": "ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø´Ø· ÙˆØ§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ØªÙƒÙˆÙŠÙ†ÙŠ.",
            "Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©": "ØªØµÙ…ÙŠÙ… Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙˆØ·Ù†ÙŠØ© Ù…ÙˆØ­Ø¯Ø© ÙˆØ±Ø¨Ø· Ù†ØªØ§Ø¦Ø¬Ù‡Ø§ Ø¨Ø®Ø·Ø· Ø§Ù„ØªØ­Ø³ÙŠÙ†."
        }
        
        clusters = {
            "ØªØ¹Ù„ÙŠÙ…": {"Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ³","Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬","Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ"},
            "ØªÙ‚ÙŠÙŠÙ…": {"Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ØªØ±Ø¨ÙˆÙŠ","Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©","Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ"},
            "Ø£Ø³Ø±Ø© ÙˆÙ…Ø¬ØªÙ…Ø¹": {"Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø§Ø³Ø±Ø©","Ø§Ù„Ø´Ø±Ø§ÙƒØ© Ù…Ø¹ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ","ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø´Ø®ØµÙŠØ©"},
            "Ø¨ÙŠØ¦Ø© ÙˆØªØ¬Ù‡ÙŠØ²": {"Ø§Ù„Ù…Ø±Ø§ÙÙ‚ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØ§Ù„Ù…Ø¨Ø§Ù†ÙŠ","Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø§Ù„Ù…Ø¯Ø§Ø±Ø³"}
        }

        # 6. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© (Feature Importance)
        weights = model.layers[0].get_weights()[0]
        importances = np.mean(np.abs(weights), axis=1)
        importances = importances / importances.sum()
        feature_importance_map = {indicator_names[i]: float(importances[i]) for i in range(len(indicator_names))}

        return model, scaler_X, scaler_y, indicator_names, recommendations_map, execution_plan_map, clusters, feature_importance_map
    
    except Exception as e:
        # Ø¥Ø°Ø§ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø¤Ù‚ØªØŒ Ù†Ø¸Ù‡Ø± Ø§Ù„Ø®Ø·Ø£ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
        st.error(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„ (Ø¨Ø¹Ø¯ ÙÙƒ Ø§Ù„Ø¶ØºØ·). Ø§Ù„Ø®Ø·Ø£: {e}")
        return default_return

# ÙŠØ¬Ø¨ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù‡Ù†Ø§:
model, scaler_X, scaler_y, indicator_names, recommendations_map, execution_plan_map, clusters, feature_importance_map = load_assets_secure()

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¢Ø²Ø± (Ù…Ù† Ø§Ù„Ø¬Ø²Ø¡ 8 ÙÙŠ ÙƒÙˆØ¯Ùƒ Ø§Ù„Ø£ØµÙ„ÙŠ)
def synergy_multiplier(selected_inds, clusters):
    selected = set(selected_inds)
    hits = {c: len(selected & members) for c, members in clusters.items()}
    same_cluster_boost = sum(1 for _, v in hits.items() if v >= 2) * 0.08
    multi_cluster_boost = sum(1 for _, v in hits.items() if v >= 1) * 0.03
    m = 1.0 + same_cluster_boost + multi_cluster_boost
    return min(m, 1.25)


# ======================================================================
# -------------------- 2. ÙˆØ¸ÙŠÙØ© Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ --------------------
# ======================================================================

def run_prediction_and_analysis(input_values, model, scaler_X, scaler_y, indicator_names, clusters, feature_importance_map):
    
    # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø¨Ù„ Ø§Ù„ØªØ´ØºÙŠÙ„
    if model is None:
        st.warning("Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ø¬Ø§Ù‡Ø² Ø¨Ø³Ø¨Ø¨ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¢Ù…Ù†. ÙŠØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø®Ø·Ø£.")
        return None, None, None, None, None, None, None, None

    # 1. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
    input_array = np.array([input_values]).astype(float)
    
    # 2. Ø§Ù„ØªØ·Ø¨ÙŠØ¹ (Normalization)
    try:
        X_scaled = scaler_X.transform(input_array)
    except ValueError as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠØ¹: ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù†Ùƒ ØªÙØ¯Ø®Ù„ 12 Ù‚ÙŠÙ…Ø© Ø¨Ø§Ù„Ø¶Ø¨Ø·. {e}")
        return None, None, None, None, None, None, None, None

    # 3. Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨
    y_scaled = model.predict(X_scaled, verbose=0)
    y_pred_orig = scaler_y.inverse_transform(y_scaled).flatten()[0]
    
    # 4. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© (Top 5 Risks)
    risks_sorted = sorted([(indicator_names[j], X_scaled[0, j]) for j in range(len(indicator_names))], key=lambda x: x[1])
    top_inds = [r[0] for r in risks_sorted[:5]]

    # 5. Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø«Ø± ÙˆØ§Ù„Ù…ÙƒØ§Ø³Ø¨ (Synergy and Gain)
    m_synergy = synergy_multiplier(top_inds, clusters)
    
    total_gain = y_pred_orig * 0.1 * sum([feature_importance_map[ind] for ind in top_inds]) * m_synergy
    
    # Ø­Ø³Ø§Ø¨ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
    rank_strong = max(1.0, y_pred_orig - total_gain)
    rank_partial = max(1.0, y_pred_orig - total_gain * 0.6)
    rank_weak = max(1.0, y_pred_orig - total_gain * 0.3)
    
    # 6. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ø´Ø± Ø°Ùˆ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ù‚ØµÙˆÙ‰ (Rank 1 from Impact/Cost)
    impact_cost_rows = []
    for ind, norm_val in risks_sorted:
        importance = feature_importance_map.get(ind, 0.0)
        base_component = max(1.0 - float(norm_val), 0.02)
        weight = base_component * importance
        impact_cost_rows.append({
            "Ø§Ù„Ù…Ø¤Ø´Ø±": ind,
            "Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø«Ø± Ø¥Ù„Ù‰ Ø§Ù„ØªÙƒÙ„ÙØ©": weight / 2 # Ø§Ù„ØªÙƒÙ„ÙØ© Ø«Ø§Ø¨ØªØ© (2)
        })
    df_impact = pd.DataFrame(impact_cost_rows)
    df_impact["ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©"] = df_impact["Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø«Ø± Ø¥Ù„Ù‰ Ø§Ù„ØªÙƒÙ„ÙØ©"].rank(ascending=False, method="dense").astype(int)
    
    priority_1_indicator = df_impact[df_impact["ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©"] == 1]['Ø§Ù„Ù…Ø¤Ø´Ø±'].iloc[0]
    
    return y_pred_orig, rank_strong, rank_partial, rank_weak, total_gain, m_synergy, top_inds, priority_1_indicator


# ======================================================================
# -------------------- 3. ÙˆØ§Ø¬Ù‡Ø© Streamlit --------------------
# ======================================================================

if model and indicator_names:
    st.set_page_config(layout="wide", page_title="Ù…Ù†ØµØ© Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠ")
    
    # CSS Ù„ØªØ®ØµÙŠØµ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ
    st.markdown("""
        <style>
            .arabic-font {
                font-family: 'Tahoma', sans-serif;
                direction: rtl;
                text-align: right;
            }
            .st-emotion-cache-1jm692v {
                direction: rtl;
            }
            .st-emotion-cache-1jm692v * {
                direction: rtl;
                text-align: right;
            }
            .big-font {
                font-size: 30px !important;
                font-weight: bold;
                color: #004d99; 
            }
        </style>
    """, unsafe_allow_html=True)
    
    st.markdown('<p class="arabic-font big-font">ğŸš€ Ù…Ù†ØµØ© Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠ (AI Prescriptive Agent)</p>', unsafe_allow_html=True)
    st.markdown('<p class="arabic-font">Ø£Ø¯Ø®Ù„ Ù‚ÙŠÙ… Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ù„Ø¹Ø§Ù… Ø§Ù„ØªÙ†Ø¨Ø¤ (2025-2030) ÙˆØ§Ø³ØªØ¹Ø±Ø¶ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© ÙˆØ£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„ØªØ¯Ø®Ù„.</p>', unsafe_allow_html=True)

    # --- Ù‚Ø³Ù… Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ---
    st.sidebar.markdown('<p class="arabic-font">âš™ï¸ Ø£Ø¯Ø®Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù€ 12</p>', unsafe_allow_html=True)
    
    input_cols = st.sidebar.columns(2)
    input_values = []
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù€ 12
    for i, ind_name in enumerate(indicator_names):
        col = input_cols[i % 2]
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ ÙˆØ§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ ÙƒÙ†Ø·Ø§Ù‚ Ù„Ù€ slider (Ø§ÙØªØ±Ø§Ø¶Ù‹Ø§ Ù…Ù† 0 Ø¥Ù„Ù‰ 100)
        # ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„Ø¨ÙŠØ§Ù†Ø§ØªÙƒ
        val = col.slider(f"{ind_name} (0-100)", 0.0, 100.0, 50.0, key=f"input_{i}")
        input_values.append(val)

    # --- ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ---
    if st.sidebar.button('ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª'):
        
        y_pred_orig, rank_strong, rank_partial, rank_weak, total_gain, m_synergy, top_inds, priority_1_indicator = run_prediction_and_analysis(
            input_values, model, scaler_X, scaler_y, indicator_names, clusters, feature_importance_map
        )
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ Ù‚ÙŠÙ… Ù…Ø®Ø±Ø¬Ø© (Ù„Ù… ÙŠØ­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ run_prediction_and_analysis)
        if y_pred_orig is not None:
            # --- Ù‚Ø³Ù… Ù„ÙˆØ­Ø© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø© (Dashboard) ---
            st.header("ğŸ¥‡ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ")
            st.markdown("---")
            
            col1, col2, col3 = st.columns(3)

            # Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ 1: Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙ†Ø¨Ø£ (Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ)
            col1.metric(
                label="Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙ†Ø¨Ø£ (Ø¨Ø¯ÙˆÙ† ØªØ¯Ø®Ù„)",
                value=f"{y_pred_orig:.2f} Ø±ØªØ¨Ø©",
                delta="ÙƒÙ„Ù…Ø§ Ù‚Ù„ Ø§Ù„Ø±Ù‚Ù… ØªØ­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡",
                delta_color="off"
            )
            
            # Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ 2: Ø§Ù„Ù…ÙƒØ³Ø¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
            col2.metric(
                label="Ù…ÙƒØ³Ø¨ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ (Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù‚ÙˆÙŠØ©)",
                value=f"+{total_gain:.2f} Ø±ØªØ¨Ø©",
                delta=f"Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ¢Ø²Ø± (M): {m_synergy:.2f}",
                delta_color="inverse"
            )

            # Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ 3: Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ù‚ØµÙˆÙ‰ Ù„Ù„ØªØ¯Ø®Ù„
            col3.metric(
                label="Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© (Ø§Ù„Ù…Ø±ØªØ¨Ø© 1)",
                value=priority_1_indicator,
                delta="Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© (Ø£Ø«Ø± / ØªÙƒÙ„ÙØ©)",
                delta_color="off"
            )
            
            st.subheader("Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©")
            
            # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª
            scenario_data = pd.DataFrame({
                'Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©': ['Ù…ØªÙ†Ø¨Ø£ (Baseline)', 'Ø¶Ø¹ÙŠÙØ©', 'Ø¬Ø²Ø¦ÙŠØ©', 'Ù‚ÙˆÙŠØ©'],
                'Ø§Ù„ØªØ±ØªÙŠØ¨': [y_pred_orig, rank_weak, rank_partial, rank_strong]
            })
            
            st.bar_chart(scenario_data.set_index('Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©').sort_values('Ø§Ù„ØªØ±ØªÙŠØ¨', ascending=False), height=350)

            # --- Ù‚Ø³Ù… Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© ---
            st.header("ğŸ“ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¶Ø¹ÙŠÙØ©")
            st.markdown("---")
            
            st.write(f"Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù…ÙƒØ³Ø¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ØŒ ÙŠØ¬Ø¨ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø®Ù…Ø³Ø© Ø§Ù„Ø£Ø¶Ø¹Ù:")
            
            recommendation_data = []
            for ind in top_inds:
                recommendation_data.append({
                    "Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„Ø¶Ø¹ÙŠÙ": ind,
                    "Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©": recommendations_map.get(ind, 'ØºÙŠØ± Ù…ØªÙˆÙØ±'),
                    "Ø®Ø·Ø© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©": execution_plan_map.get(ind, 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                })
                
            df_recs = pd.DataFrame(recommendation_data)
            st.table(df_recs.set_index('Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„Ø¶Ø¹ÙŠÙ'))
