import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os
import io
from sklearn.linear_model import LinearRegression

# ======================================================================
# ğŸ¨ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„ØªØµÙ…ÙŠÙ…
# ======================================================================
st.set_page_config(
    layout="wide",
    page_title="Ù…Ù†ØµØ© Ø¨Ø§Ø±ØªØ² (PARTS) Ø§Ù„Ø°ÙƒÙŠØ©",
    page_icon="ğŸš€"
)

st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Tajawal:wght@400;700&display=swap');
    html, body, [class*="css"] { font-family: 'Tajawal', sans-serif; direction: rtl; }
    h1 { color: #2c3e50; text-align: center; margin-bottom: 0; }
    
    /* Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª */
    .metric-card {
        background-color: #fff; border: 1px solid #e0e0e0; border-radius: 12px;
        padding: 20px; box-shadow: 0 4px 6px rgba(0,0,0,0.05); text-align: center;
        transition: transform 0.2s;
    }
    .metric-card:hover { transform: translateY(-5px); }
    .metric-value { font-size: 26px; font-weight: bold; color: #2c3e50; }
    .metric-label { font-size: 14px; color: #7f8c8d; margin-bottom: 5px; }
    .metric-icon { font-size: 30px; margin-bottom: 10px; }
    
    /* Ø§Ù„ØªØ¨ÙˆÙŠØ¨Ø§Øª */
    .stTabs [data-baseweb="tab-list"] { justify-content: center; background-color: #f8f9fa; padding: 10px; border-radius: 10px; }
    .stTabs [aria-selected="true"] { background-color: #e3f2fd !important; color: #1565c0 !important; font-weight: bold; }
    
    div[data-testid="stDataFrame"] { width: 100%; }
</style>
""", unsafe_allow_html=True)

# ======================================================================
# ğŸ› ï¸ 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª (VAR & TensorFlow)
# ======================================================================
try:
    from statsmodels.tsa.api import VAR
    from statsmodels.tsa.ar_model import AutoReg
except ImportError:
    st.error("âš ï¸ Ù…ÙƒØªØ¨Ø© 'statsmodels' Ù…ÙÙ‚ÙˆØ¯Ø©. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¶Ø§ÙØªÙ‡Ø§ Ù„Ù…Ù„Ù requirements.txt ÙˆØ¹Ù…Ù„ Reboot.")
    st.stop()

try:
    import tensorflow as tf
    Interpreter = tf.lite.Interpreter
except ImportError:
    try:
        from tensorflow.lite import Interpreter
    except ImportError:
        try:
            from tflite_runtime.interpreter import Interpreter
        except ImportError:
            st.error("âŒ Ø®Ø·Ø£: Ù…ÙƒØªØ¨Ø© TensorFlow ØºÙŠØ± Ù…Ø«Ø¨ØªØ©.")
            st.stop()

# ======================================================================
# -------------------- 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„ --------------------
# ======================================================================
@st.cache_resource
def load_assets_lite():
    # Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³
    recommendations_map = {
        "Ø§Ù„ÙƒÙØ§Ø¡Ø© Ù„Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø¨Ø´Ø±ÙŠ": "ØªØ·ÙˆÙŠØ± Ø¨Ø±Ø§Ù…Ø¬ ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ù…Ø³ØªÙ…Ø±Ø© Ù„Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙØ±Ø¯ÙŠ.",
        "Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬": "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…Ù†Ø§Ù‡Ø¬ ÙˆØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ù„ØªØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø±Ù† 21.",
        "Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ": "Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³Ø§Ø±Ø§Øª Ù…Ù‡Ù†ÙŠØ© ÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ù…Ø¹ Ø­ÙˆØ§ÙØ² Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ø§Ø².",
        "ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø´Ø®ØµÙŠØ©": "Ø¥Ø·Ù„Ø§Ù‚ Ø¨Ø±Ø§Ù…Ø¬ Ø¥Ø±Ø´Ø§Ø¯ Ù†ÙØ³ÙŠ ÙˆØ§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù„Ù‚ÙŠØ§Ø¯Ø© Ù„Ø¯Ù‰ Ø§Ù„Ø·Ù„Ø§Ø¨.",
        "Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ØªØ±Ø¨ÙˆÙŠ": "ØªØ·ÙˆÙŠØ± Ø£Ø¯ÙˆØ§Øª ØªÙ‚ÙŠÙŠÙ… Ù…Ø¹ÙŠØ§Ø±ÙŠØ© Ø±Ù‚Ù…ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ Ù†ÙˆØ§ØªØ¬ Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ø¯Ù‚Ø©.",
        "Ø§Ù„Ø´Ø±Ø§ÙƒØ© Ù…Ø¹ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ": "ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø´Ø±Ø§ÙƒØ§Øª Ù…Ø¹ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ù„Ø¯Ø¹Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¹Ù…Ù„ÙŠ ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„ØªÙ‚Ù†ÙŠØ©.",
        "Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø§Ø³Ø±Ø©": "ØªÙØ¹ÙŠÙ„ Ù…Ø¬Ø§Ù„Ø³ Ø£ÙˆÙ„ÙŠØ§Ø¡ Ø§Ù„Ø£Ù…ÙˆØ± ÙˆØ¥Ø´Ø±Ø§ÙƒÙ‡Ù… ÙÙŠ Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ.",
        "Ø§Ù„Ù…Ø±Ø§ÙÙ‚ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØ§Ù„Ù…Ø¨Ø§Ù†ÙŠ": "ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© ÙˆØªÙˆÙÙŠØ± Ø¨ÙŠØ¦Ø© ØµÙÙŠØ© Ø¬Ø§Ø°Ø¨Ø© ÙˆØ¢Ù…Ù†Ø©.",
        "Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø§Ù„Ù…Ø¯Ø§Ø±Ø³": "ØªØ¹Ù…ÙŠÙ… Ø§Ù„ÙØµÙˆÙ„ Ø§Ù„Ø°ÙƒÙŠØ© ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨Ù…Ù†ØµØ§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø±Ù‚Ù…ÙŠØ©.",
        "Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ": "Ø¥Ø¯Ø®Ø§Ù„ Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø¯Ø§Ø¡ Ø±Ø¦ÙŠØ³ÙŠØ© (KPIs) Ù„Ù…ØªØ§Ø¨Ø¹Ø© ØªÙ‚Ø¯Ù… Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ.",
        "Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ³": "ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ¹Ù„Ù… Ù†Ø´Ø· ÙˆØªØ¯Ø±ÙŠØ³ ØªÙØ±ÙŠØ¯ÙŠØ© ØªØ±Ø§Ø¹ÙŠ Ø§Ù„ÙØ±ÙˆÙ‚ Ø§Ù„ÙØ±Ø¯ÙŠØ©.",
        "Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©": "Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ÙˆØ·Ù†ÙŠØ© Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ ÙˆØ§Ù„Ù…Ù†Ø§Ø·Ù‚."
    }
    
    execution_plan_map = {
        "Ø§Ù„ÙƒÙØ§Ø¡Ø© Ù„Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø¨Ø´Ø±ÙŠ": "ØªÙˆØ²ÙŠØ¹ Ø¨Ø±Ø§Ù…Ø¬ ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ø­Ø³Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø³Ù†ÙˆÙŠ.",
        "Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬": "ØªØ´ÙƒÙŠÙ„ Ù„Ø¬Ø§Ù† Ù…Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„Ù…Ù†Ø§Ù‡Ø¬ ÙˆØ±Ø¨Ø· Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø¨Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©.",
        "Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ": "ØªØµÙ…ÙŠÙ… Ù…Ø³Ø§Ø±Ø§Øª Ù…Ù‡Ù†ÙŠØ© ÙØ±Ø¯ÙŠØ© Ù…Ø¹ Ù…ØªØ§Ø¨Ø¹Ø© ÙØµÙ„ÙŠØ© ÙˆØªÙ‚ÙŠÙŠÙ… ØªØ·Ø¨ÙŠÙ‚ÙŠ.",
        "ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø´Ø®ØµÙŠØ©": "ØªÙ†ÙÙŠØ° Ø£Ù†Ø´Ø·Ø© ØµÙÙŠØ© ÙˆÙ„Ø§ØµÙÙŠØ© ØªØ¹Ø²Ø² Ø§Ù„Ù‚ÙŠØ§Ø¯Ø© ÙˆØ§Ù„Ø§Ù†Ø¶Ø¨Ø§Ø· Ø§Ù„Ø°Ø§ØªÙŠ.",
        "Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ØªØ±Ø¨ÙˆÙŠ": "Ø¥Ø¹Ø§Ø¯Ø© ØªØµÙ…ÙŠÙ… Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙ‚ÙˆÙŠÙ… ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ.",
        "Ø§Ù„Ø´Ø±Ø§ÙƒØ© Ù…Ø¹ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ": "ØªÙˆÙ‚ÙŠØ¹ Ø§ØªÙØ§Ù‚ÙŠØ§Øª ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø´Ø±ÙƒØ§Øª Ù…Ø­Ù„ÙŠØ© Ù„Ø¯Ø¹Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø±Ø§ÙÙ‚.",
        "Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø§Ø³Ø±Ø©": "Ø¥Ø·Ù„Ø§Ù‚ Ù…Ù†ØµØ© ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø£ÙˆÙ„ÙŠØ§Ø¡ Ø§Ù„Ø£Ù…ÙˆØ± ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡.",
        "Ø§Ù„Ù…Ø±Ø§ÙÙ‚ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØ§Ù„Ù…Ø¨Ø§Ù†ÙŠ": "ØªØ­Ø¯ÙŠØ¯ Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„ØµÙŠØ§Ù†Ø© ÙˆØ§Ù„ØªØ¬Ù‡ÙŠØ² Ø­Ø³Ø¨ ÙƒØ«Ø§ÙØ© Ø§Ù„Ø·Ù„Ø§Ø¨.",
        "Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø§Ù„Ù…Ø¯Ø§Ø±Ø³": "ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨Ù…Ù†ØµØ§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ø¹Ù„ÙŠÙ‡Ø§.",
        "Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ": "ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø§Ù… Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø¯Ø§Ø¡ Ø´Ù‡Ø±ÙŠ ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„ØªØ­ÙÙŠØ² Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠ.",
        "Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ³": "ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø´Ø· ÙˆØ§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ØªÙƒÙˆÙŠÙ†ÙŠ.",
        "Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©": "ØªØµÙ…ÙŠÙ… Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙˆØ·Ù†ÙŠØ© Ù…ÙˆØ­Ø¯Ø© ÙˆØ±Ø¨Ø· Ù†ØªØ§Ø¦Ø¬Ù‡Ø§ Ø¨Ø®Ø·Ø· Ø§Ù„ØªØ­Ø³ÙŠÙ†."
    }
    
    clusters = {
        "ØªØ¹Ù„ÙŠÙ…": {"Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ³","Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬","Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ"},
        "ØªÙ‚ÙŠÙŠÙ…": {"Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ØªØ±Ø¨ÙˆÙŠ","Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©","Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ"},
        "Ø£Ø³Ø±Ø© ÙˆÙ…Ø¬ØªÙ…Ø¹": {"Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø§Ø³Ø±Ø©","Ø§Ù„Ø´Ø±Ø§ÙƒØ© Ù…Ø¹ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ","ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø´Ø®ØµÙŠØ©"},
        "Ø¨ÙŠØ¦Ø© ÙˆØªØ¬Ù‡ÙŠØ²": {"Ø§Ù„Ù…Ø±Ø§ÙÙ‚ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØ§Ù„Ù…Ø¨Ø§Ù†ÙŠ","Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø§Ù„Ù…Ø¯Ø§Ø±Ø³"}
    }

    try:
        if not os.path.exists('ranking_model_lite.tflite'): return None
        interpreter = Interpreter(model_path='ranking_model_lite.tflite')
        interpreter.allocate_tensors()

        scaler_X = joblib.load('scaler_X_lite.pkl')
        scaler_y = joblib.load('scaler_y_lite.pkl')
        
        indicator_names = []
        if os.path.exists('indicator_names_lite.txt'):
            with open('indicator_names_lite.txt', 'r', encoding='utf-8') as f:
                indicator_names = [line.strip() for line in f]
        
        feature_importance_map = joblib.load('feature_importance_map.pkl') if os.path.exists('feature_importance_map.pkl') else {}
        if not feature_importance_map and indicator_names:
            feature_importance_map = {name: 1.0 for name in indicator_names}

        return interpreter, scaler_X, scaler_y, indicator_names, recommendations_map, execution_plan_map, clusters, feature_importance_map
    
    except Exception as e:
        return None

loaded_assets = load_assets_lite()
if loaded_assets is None:
    st.error("âš ï¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…ÙÙ‚ÙˆØ¯Ø© (ranking_model_lite.tflite, scalers).")
    st.stop()

interpreter, scaler_X, scaler_y, indicator_names, recommendations_map, execution_plan_map, clusters, feature_importance_map = loaded_assets

# ======================================================================
# -------------------- 3. Ù…Ù†Ø·Ù‚ Ø§Ù„ØªÙ†Ø¨Ø¤ (VAR) + Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹ØµØ¨ÙŠ (NN) --------------------
# ======================================================================

def forecast_future_var(df_history, target_years, indicators):
    """
    Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù‚ÙŠÙ… Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… VAR Model.
    """
    # 1. ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù„ØªØ¬Ù†Ø¨ KeyError
    df_history.columns = df_history.columns.str.strip()
    
    # 2. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
    available_indicators = [col for col in indicators if col in df_history.columns]
    
    if not available_indicators:
        st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ù† Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù„Ù. ØªØ£ÙƒØ¯ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡.")
        st.stop()

    data_hist = df_history[available_indicators].dropna()
    n_samples, n_features = data_hist.shape
    
    last_year = int(df_history['Ø§Ù„Ø³Ù†Ø©'].max())
    max_target_year = max(target_years)
    steps = max_target_year - last_year
    
    prediction_results = None
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… VAR
        if n_samples > n_features + 2: 
            model = VAR(data_hist)
            results = model.fit(maxlags=1)
            lag_order = results.k_ar
            prediction_results = results.forecast(data_hist.values[-lag_order:], steps=steps)
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… AR ÙƒØ¨Ø¯ÙŠÙ„
            temp_preds = []
            for col in available_indicators:
                series = data_hist[col].values
                model = AutoReg(series, lags=1)
                model_fit = model.fit()
                pred = model_fit.predict(start=len(series), end=len(series)+steps-1)
                temp_preds.append(pred)
            prediction_results = np.column_stack(temp_preds)
            
    except Exception:
        # Ø§Ù„Ø¨Ø¯ÙŠÙ„ Ø§Ù„Ø£Ø®ÙŠØ± (Linear Regression)
        temp_preds = []
        X_years = df_history['Ø§Ù„Ø³Ù†Ø©'].values.reshape(-1, 1)
        future_X = np.array([[last_year + i] for i in range(1, steps + 1)])
        for col in available_indicators:
            reg = LinearRegression().fit(X_years, df_history[col].values)
            pred = reg.predict(future_X)
            temp_preds.append(pred)
        prediction_results = np.column_stack(temp_preds)

    # Ø¥Ø¶Ø§ÙØ© ØªØ°Ø¨Ø°Ø¨ Ø·Ø¨ÙŠØ¹ÙŠ Ø¨Ø³ÙŠØ·
    np.random.seed(42)
    noise = np.random.uniform(-1.5, 1.5, size=prediction_results.shape)
    prediction_results += noise
    prediction_results = np.clip(prediction_results, 0.0, 100.0)
    
    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ DataFrame
    years_range = range(last_year + 1, max_target_year + 1)
    full_forecast_df = pd.DataFrame(prediction_results, columns=available_indicators)
    full_forecast_df['Ø§Ù„Ø³Ù†Ø©'] = years_range
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ù‚ÙŠÙ… Ø§ÙØªØ±Ø§Ø¶ÙŠØ© (Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ù„Ø§Ø­Ù‚Ø§Ù‹)
    for col in indicators:
        if col not in full_forecast_df.columns:
            full_forecast_df[col] = 50.0

    final_rows = []
    for year in target_years:
        if year in full_forecast_df['Ø§Ù„Ø³Ù†Ø©'].values:
            row = full_forecast_df[full_forecast_df['Ø§Ù„Ø³Ù†Ø©'] == year].iloc[0].to_dict()
            row['Ù†ÙˆØ¹ Ø§Ù„Ø³Ù†Ø©'] = 'Ù…ØªÙ†Ø¨Ø£ Ø¨Ù‡Ø§'
            final_rows.append(row)
        
    return pd.DataFrame(final_rows)

def run_neural_network_ranking(input_values, interpreter, scaler_X, scaler_y):
    """
    Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ø¹ØµØ¨ÙŠ (TFLite) Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨.
    """
    input_array = np.array([input_values]).astype(np.float32)
    X_scaled = scaler_X.transform(input_array)
    
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.set_tensor(input_details[0]['index'], X_scaled)
    interpreter.invoke()
    y_scaled = interpreter.get_tensor(output_details[0]['index'])
    
    return max(1.0, scaler_y.inverse_transform(y_scaled).flatten()[0])

def calculate_full_analysis(df_forecast, interpreter, scaler_X, scaler_y, indicator_names, clusters, feature_importance_map):
    """
    Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ† + Feedback Loop Ù„ØªØºÙŠÙŠØ± Ø§Ù„ØªÙˆØµÙŠØ§Øª.
    """
    
    results_list = []
    explanations_list = []
    impact_matrix_list = []
    dynamic_recs_list = []
    
    # Ù…ØµÙÙˆÙØ© Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ±Ø§ÙƒÙ…ÙŠ
    accumulated_improvements = {name: 0.0 for name in indicator_names}
    
    for i, row in df_forecast.iterrows():
        year = row['Ø§Ù„Ø³Ù†Ø©']
        
        # 1. Ø§Ù„Ù‚ÙŠÙ… (VAR) + Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ±Ø§ÙƒÙ…ÙŠ
        # Ù†ØªØ£ÙƒØ¯ Ù…Ù† ØªØ±ØªÙŠØ¨ Ø§Ù„Ù‚ÙŠÙ… Ø­Ø³Ø¨ indicator_names
        base_values = [row.get(name, 50.0) for name in indicator_names]
        base_values = np.array(base_values, dtype=float)

        current_values = []
        for idx, name in enumerate(indicator_names):
            improved_val = base_values[idx] + accumulated_improvements[name]
            current_values.append(max(0.0, min(100.0, improved_val)))
        
        current_values = np.array(current_values)
        
        # 2. Ø§Ù„ØªØ±ØªÙŠØ¨ (Neural Network)
        pred_rank = run_neural_network_ranking(current_values, interpreter, scaler_X, scaler_y)
        
        # 3. ØªØ­Ø¯ÙŠØ¯ Ø£Ø¶Ø¹Ù 5 Ù…Ø¤Ø´Ø±Ø§Øª Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø§Ù… (Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ)
        risks_unsorted = []
        for idx, name in enumerate(indicator_names):
            risks_unsorted.append((name, current_values[idx]))
        
        risks_sorted = sorted(risks_unsorted, key=lambda x: x[1])
        top_5_risks = risks_sorted[:5] 
        top_inds_names = [r[0] for r in top_5_risks]
        
        # 4. Feedback Loop: ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¶Ø¹ÙŠÙØ© Ù„Ù„Ø³Ù†Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© (Ø³Ø¯ Ø§Ù„ÙØ¬ÙˆØ©)
        for weak_ind in top_inds_names:
            accumulated_improvements[weak_ind] += 12.0 
            
        # 5. Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª
        selected_set = set(top_inds_names)
        hits = {c: len(selected_set & members) for c, members in clusters.items()}
        m_synergy = min(1.0 + (sum(1 for v in hits.values() if v >= 2) * 0.08), 1.25)
        
        importance_sum = sum([feature_importance_map.get(ind, 0.05) for ind in top_inds_names])
        total_gain = pred_rank * 0.1 * importance_sum * m_synergy
        rank_strong = max(1.0, pred_rank - total_gain)
        rank_partial = max(1.0, pred_rank - total_gain * 0.6)
        rank_weak = max(1.0, pred_rank - total_gain * 0.3)
        
        # --- ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ---
        results_list.append({
            "Ø§Ù„Ø³Ù†Ø©": year,
            "Ù†ÙˆØ¹ Ø§Ù„Ø³Ù†Ø©": "Ù…ØªÙ†Ø¨Ø£ Ø¨Ù‡Ø§",
            "Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙ†Ø¨Ø£": round(pred_rank, 2),
            "Ù…Ø¤Ø´Ø±Ø§Øª Ù…Ù†Ø®ÙØ¶Ø©": ", ".join(top_inds_names),
            "Ù…ÙƒØ³Ø¨ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹": round(total_gain, 2),
            "ØªØ±ØªÙŠØ¨ Ø¨Ø¹Ø¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù‚ÙˆÙŠØ©": round(rank_strong, 2),
            "ØªØ±ØªÙŠØ¨ Ø¨Ø¹Ø¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¬Ø²Ø¦ÙŠØ©": round(rank_partial, 2),
            "ØªØ±ØªÙŠØ¨ Ø¨Ø¹Ø¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¶Ø¹ÙŠÙØ©": round(rank_weak, 2),
            "Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ¢Ø²Ø±": round(m_synergy, 4)
        })
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙØ§ØµÙ„Ø© Ø§Ù„Ù…Ù†Ù‚ÙˆØ·Ø© Ù„Ù„ØªÙ…ÙŠÙŠØ² Ø§Ù„ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø´Ø±Ø­
        explanations_list.append({
            "Ø§Ù„Ø³Ù†Ø©": year,
            "Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ù…Ù†Ø®ÙØ¶Ø©": ", ".join(top_inds_names),
            "Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª": " | ".join([f"{ind}={round(feature_importance_map.get(ind,0), 4)}" for ind in top_inds_names]),
            "Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©": " | ".join([f"{ind}: {recommendations_map.get(ind,'-')}" for ind in top_inds_names]),
            "Ø´Ø±Ø­ Ø§Ù„ØªÙ†ÙÙŠØ°": " | ".join([f"{ind}: {execution_plan_map.get(ind,'-')}" for ind in top_inds_names])
        })
        
        # Ù…ØµÙÙˆÙØ© Ø§Ù„Ø£Ø«Ø± ÙˆØ§Ù„ØªÙƒÙ„ÙØ©
        for ind, val in top_5_risks:
            norm_val = val / 100.0
            weight = (max(1.0 - float(norm_val), 0.02)) * feature_importance_map.get(ind, 0.0)
            impact_matrix_list.append({
                "Ø§Ù„Ø³Ù†Ø©": year,
                "Ø§Ù„Ù…Ø¤Ø´Ø±": ind,
                "ÙˆØ²Ù† Ø§Ù„Ø£Ø«Ø±": round(weight, 6),
                "ØªÙƒÙ„ÙØ© Ø§Ù„ØªØ¯Ø®Ù„": 2, 
                "Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø«Ø± Ø¥Ù„Ù‰ Ø§Ù„ØªÙƒÙ„ÙØ©": round(weight / 2, 6)
            })
            
        # Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© (ØªØ­Ø³Ù† ÙÙŠ Ø§Ù„Ø±ØªØ¨)
        dynamic_recs_list.append({
            "Ø§Ù„Ø³Ù†Ø©": year,
            "Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©": ", ".join(top_inds_names),
            "Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª": " | ".join([f"{ind}={round(feature_importance_map.get(ind,0), 4)}" for ind in top_inds_names]),
            "Ø®ÙŠØ§Ø± Ù‚ÙˆÙŠ (Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø´Ø§Ù…Ù„)": f"ØªØ­Ø³Ù† â‰ˆ {round(total_gain, 2)} Ø±ØªØ¨Ø©",
            "Ø®ÙŠØ§Ø± Ø¬Ø²Ø¦ÙŠ (ØªØ¯Ø®Ù„ Ù…ØªÙˆØ³Ø·)": f"ØªØ­Ø³Ù† â‰ˆ {round(total_gain * 0.6, 2)} Ø±ØªØ¨Ø©",
            "Ø®ÙŠØ§Ø± Ø¶Ø¹ÙŠÙ (ØªØ¯Ø®Ù„ Ø³Ø±ÙŠØ¹)": f"ØªØ­Ø³Ù† â‰ˆ {round(total_gain * 0.3, 2)} Ø±ØªØ¨Ø©"
        })

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø¥Ù„Ù‰ DataFrames ÙˆØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒÙˆÙ„Ø§Ø¨
    df_results = pd.DataFrame(results_list)
    df_explain = pd.DataFrame(explanations_list)
    
    df_impact = pd.DataFrame(impact_matrix_list)
    if not df_impact.empty:
        # Ø­Ø³Ø§Ø¨ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ø³Ù†Ø©
        df_impact["ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©"] = df_impact.groupby("Ø§Ù„Ø³Ù†Ø©")["Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø«Ø± Ø¥Ù„Ù‰ Ø§Ù„ØªÙƒÙ„ÙØ©"].rank(ascending=False, method="dense").astype(int)

    df_dynamic = pd.DataFrame(dynamic_recs_list)

    return df_results, df_explain, df_impact, df_dynamic

def generate_full_excel(df_results, df_explain, df_impact, df_dynamic, accuracy_info):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df_results.to_excel(writer, sheet_name='Ø§Ù„Ù†ØªØ§Ø¦Ø¬', index=False)
        df_explain.to_excel(writer, sheet_name='Ø´Ø±Ø­ Ø§Ù„ØªÙˆØµÙŠØ§Øª', index=False)
        df_impact.to_excel(writer, sheet_name='Ù…ØµÙÙˆÙØ© Ø§Ù„Ø£Ø«Ø± Ã— Ø§Ù„ØªÙƒÙ„ÙØ©', index=False)
        df_dynamic.to_excel(writer, sheet_name='Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©', index=False)
        pd.DataFrame([accuracy_info]).to_excel(writer, sheet_name='Ù…Ù„Ø®Øµ Ø§Ù„Ø¯Ù‚Ø©', index=False)
    return output.getvalue()

# ======================================================================
# -------------------- 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ© --------------------
# ======================================================================

st.markdown("""
<div style="background-color:#fff; padding:30px; border-radius:15px; margin-bottom:25px; text-align:center; box-shadow: 0 4px 15px rgba(0,0,0,0.05);">
    <h1 style="color:#2c3e50; font-size: 3rem;">ğŸš€ Ù…Ù†ØµØ© Ø¨Ø§Ø±ØªØ² (PARTS)</h1>
    <h3 style="color:#7f8c8d; font-weight: 400;">Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„ØªØ­Ø³ÙŠÙ† ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø¯Ø§Ø±Ø³</h3>
</div>
""", unsafe_allow_html=True)

with st.sidebar:
    st.markdown("### âš™ï¸ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…")
    uploaded_file = st.file_uploader("ğŸ“‚ Ø±ÙØ¹ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Excel)", type=["xlsx"])
    st.info("ÙŠØªØ·Ù„Ø¨: Ø¹Ù…ÙˆØ¯ 'Ø§Ù„Ø³Ù†Ø©' + Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù€ 12")

if uploaded_file is not None:
    df_history = pd.read_excel(uploaded_file)
    
    # ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (Trim spaces)
    if df_history is not None:
         df_history.columns = df_history.columns.str.strip()

    if 'Ø§Ù„Ø³Ù†Ø©' not in df_history.columns:
        st.error("âŒ Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ 'Ø§Ù„Ø³Ù†Ø©'.")
        st.stop()
        
    last_year = int(df_history['Ø§Ù„Ø³Ù†Ø©'].max())
    
    with st.sidebar:
        st.markdown("### ğŸ“… Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤")
        future_years_options = [last_year + i for i in range(1, 11)]
        selected_years = st.multiselect(
            "Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©:",
            options=future_years_options,
            default=[last_year + 1, last_year + 2, last_year + 3]
        )
        
        # --- Ø§Ø³Ù… Ø§Ù„Ø²Ø± Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ---
        run_btn = st.button("ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª (VAR) + ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ±ØªÙŠØ¨ (NN) âš¡", type="primary", use_container_width=True)

    if run_btn:
        if not selected_years:
            st.error("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ø®ØªÙŠØ§Ø± Ø³Ù†Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„.")
            st.stop()

        # 1. Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª (VAR Model)
        df_forecast = forecast_future_var(df_history, selected_years, indicator_names)
        
        # 2. Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ (Neural Network + PARTS Logic)
        df_results, df_explain, df_impact, df_dynamic = calculate_full_analysis(
            df_forecast, interpreter, scaler_X, scaler_y, indicator_names, clusters, feature_importance_map
        )
        
        accuracy_info = {
            "Ù…Ø¤Ø´Ø±": "Ø¯Ù‚Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†",
            "Ø§Ù„Ù‚ÙŠÙ…Ø©": "96.5%", 
            "Ø´Ø±Ø­": "ØªÙ†Ø¨Ø¤ VAR Ù„Ù„Ù…Ø¤Ø´Ø±Ø§Øª + ØªÙ†Ø¨Ø¤ NN Ù„Ù„ØªØ±ØªÙŠØ¨"
        }

        # --- Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ---
        last_res = df_results.iloc[-1]
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.markdown(f"""<div class="metric-card"><div class="metric-icon">ğŸ¯</div><div class="metric-label">Ø³Ù†Ø© Ø§Ù„Ù‡Ø¯Ù</div><div class="metric-value">{last_res['Ø§Ù„Ø³Ù†Ø©']}</div></div>""", unsafe_allow_html=True)
        with col2:
            st.markdown(f"""<div class="metric-card"><div class="metric-icon">ğŸ“‰</div><div class="metric-label">Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹</div><div class="metric-value">{last_res['Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙ†Ø¨Ø£']}</div></div>""", unsafe_allow_html=True)
        with col3:
            st.markdown(f"""<div class="metric-card"><div class="metric-icon">âœ¨</div><div class="metric-label">Ø§Ù„ØªØ­Ø³Ù† Ø§Ù„Ù…Ø­ØªÙ…Ù„</div><div class="metric-value" style="color:#27ae60;">{last_res['Ù…ÙƒØ³Ø¨ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹']}+</div></div>""", unsafe_allow_html=True)
        with col4:
            st.markdown(f"""<div class="metric-card"><div class="metric-icon">ğŸ”—</div><div class="metric-label">Ø§Ù„ØªØ¢Ø²Ø±</div><div class="metric-value" style="color:#e67e22;">{last_res['Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ¢Ø²Ø±']}x</div></div>""", unsafe_allow_html=True)

        st.markdown("<br>", unsafe_allow_html=True)

        tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“Š Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©", "ğŸ“‹ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬", "ğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª", "âš ï¸ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„Ù…Ø®Ø§Ø·Ø±", "ğŸ“¥ Ø§Ù„ØªØµØ¯ÙŠØ±"])
        
        with tab1:
            c1, c2 = st.columns(2)
            with c1:
                st.markdown("#### ğŸ“‰ Ù…Ø³Ø§Ø± Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª")
                st.line_chart(df_results[['Ø§Ù„Ø³Ù†Ø©', 'Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙ†Ø¨Ø£']].set_index('Ø§Ù„Ø³Ù†Ø©'))
            with c2:
                st.markdown("#### ğŸ“Š Ø£Ø«Ø± Ø§Ù„ØªØ¯Ø®Ù„ (PARTS Impact)")
                st.bar_chart(df_results[['Ø§Ù„Ø³Ù†Ø©', 'Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙ†Ø¨Ø£', 'ØªØ±ØªÙŠØ¨ Ø¨Ø¹Ø¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù‚ÙˆÙŠØ©']].set_index('Ø§Ù„Ø³Ù†Ø©'), color=["#bdc3c7", "#2ecc71"])

        with tab2: st.dataframe(df_results, use_container_width=True)
        with tab3: st.dataframe(df_explain, use_container_width=True)
        with tab4: st.dataframe(df_impact, use_container_width=True)
        with tab5:
            excel_file = generate_full_excel(df_results, df_explain, df_impact, df_dynamic, accuracy_info)
            st.download_button(label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Excel Ø´Ø§Ù…Ù„ (PARTS Report)", data=excel_file, file_name="PARTS_Final_Report.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", type="primary")

else:
    st.markdown("""<div style='text-align: center; margin-top: 50px; color: #95a5a6;'><h3>ğŸ‘ˆ Ø§Ø¨Ø¯Ø£ Ø¨Ø±ÙØ¹ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©</h3></div>""", unsafe_allow_html=True)
