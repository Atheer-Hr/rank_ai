import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os
from sklearn.linear_model import LinearRegression

# ======================================================================
# ğŸ› ï¸ 1. Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ (Smart Import)
# ======================================================================
try:
    import tensorflow as tf
    Interpreter = tf.lite.Interpreter
except ImportError:
    try:
        from tensorflow.lite import Interpreter
    except ImportError:
        try:
            from tflite_runtime.interpreter import Interpreter
        except ImportError:
            st.error("âŒ Ø®Ø·Ø£: Ù…ÙƒØªØ¨Ø© TensorFlow ØºÙŠØ± Ù…Ø«Ø¨ØªØ©.")
            st.stop()

# ======================================================================
# -------------------- 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª --------------------
# ======================================================================
@st.cache_resource
def load_assets_lite():
    # Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ (Ø§Ù„ØªÙˆØµÙŠØ§Øª ÙˆØ§Ù„Ø®Ø·Ø·)
    recommendations_map = {
        "Ø§Ù„ÙƒÙØ§Ø¡Ø© Ù„Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø¨Ø´Ø±ÙŠ": "ØªØ·ÙˆÙŠØ± Ø¨Ø±Ø§Ù…Ø¬ ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ù…Ø³ØªÙ…Ø±Ø© Ù„Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙØ±Ø¯ÙŠ.",
        "Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬": "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…Ù†Ø§Ù‡Ø¬ ÙˆØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ù„ØªØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø±Ù† 21.",
        "Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ": "Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³Ø§Ø±Ø§Øª Ù…Ù‡Ù†ÙŠØ© ÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ù…Ø¹ Ø­ÙˆØ§ÙØ² Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ø§Ø².",
        "ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø´Ø®ØµÙŠØ©": "Ø¥Ø·Ù„Ø§Ù‚ Ø¨Ø±Ø§Ù…Ø¬ Ø¥Ø±Ø´Ø§Ø¯ Ù†ÙØ³ÙŠ ÙˆØ§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù„Ù‚ÙŠØ§Ø¯Ø© Ù„Ø¯Ù‰ Ø§Ù„Ø·Ù„Ø§Ø¨.",
        "Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ØªØ±Ø¨ÙˆÙŠ": "ØªØ·ÙˆÙŠØ± Ø£Ø¯ÙˆØ§Øª ØªÙ‚ÙŠÙŠÙ… Ù…Ø¹ÙŠØ§Ø±ÙŠØ© Ø±Ù‚Ù…ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ Ù†ÙˆØ§ØªØ¬ Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ø¯Ù‚Ø©.",
        "Ø§Ù„Ø´Ø±Ø§ÙƒØ© Ù…Ø¹ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ": "ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø´Ø±Ø§ÙƒØ§Øª Ù…Ø¹ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ù„Ø¯Ø¹Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¹Ù…Ù„ÙŠ ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„ØªÙ‚Ù†ÙŠØ©.",
        "Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø§Ø³Ø±Ø©": "ØªÙØ¹ÙŠÙ„ Ù…Ø¬Ø§Ù„Ø³ Ø£ÙˆÙ„ÙŠØ§Ø¡ Ø§Ù„Ø£Ù…ÙˆØ± ÙˆØ¥Ø´Ø±Ø§ÙƒÙ‡Ù… ÙÙŠ Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ.",
        "Ø§Ù„Ù…Ø±Ø§ÙÙ‚ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØ§Ù„Ù…Ø¨Ø§Ù†ÙŠ": "ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© ÙˆØªÙˆÙÙŠØ± Ø¨ÙŠØ¦Ø© ØµÙÙŠØ© Ø¬Ø§Ø°Ø¨Ø© ÙˆØ¢Ù…Ù†Ø©.",
        "Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø§Ù„Ù…Ø¯Ø§Ø±Ø³": "ØªØ¹Ù…ÙŠÙ… Ø§Ù„ÙØµÙˆÙ„ Ø§Ù„Ø°ÙƒÙŠØ© ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨Ù…Ù†ØµØ§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø±Ù‚Ù…ÙŠØ©.",
        "Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ": "Ø¥Ø¯Ø®Ø§Ù„ Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø¯Ø§Ø¡ Ø±Ø¦ÙŠØ³ÙŠØ© (KPIs) Ù„Ù…ØªØ§Ø¨Ø¹Ø© ØªÙ‚Ø¯Ù… Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ.",
        "Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ³": "ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ¹Ù„Ù… Ù†Ø´Ø· ÙˆØªØ¯Ø±ÙŠØ³ ØªÙØ±ÙŠØ¯ÙŠØ© ØªØ±Ø§Ø¹ÙŠ Ø§Ù„ÙØ±ÙˆÙ‚ Ø§Ù„ÙØ±Ø¯ÙŠØ©.",
        "Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©": "Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ÙˆØ·Ù†ÙŠØ© Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ ÙˆØ§Ù„Ù…Ù†Ø§Ø·Ù‚."
    }
    
    clusters = {
        "ØªØ¹Ù„ÙŠÙ…": {"Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ³","Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬","Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ"},
        "ØªÙ‚ÙŠÙŠÙ…": {"Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ØªØ±Ø¨ÙˆÙŠ","Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©","Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ"},
        "Ø£Ø³Ø±Ø© ÙˆÙ…Ø¬ØªÙ…Ø¹": {"Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø§Ø³Ø±Ø©","Ø§Ù„Ø´Ø±Ø§ÙƒØ© Ù…Ø¹ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ","ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø´Ø®ØµÙŠØ©"},
        "Ø¨ÙŠØ¦Ø© ÙˆØªØ¬Ù‡ÙŠØ²": {"Ø§Ù„Ù…Ø±Ø§ÙÙ‚ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØ§Ù„Ù…Ø¨Ø§Ù†ÙŠ","Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø§Ù„Ù…Ø¯Ø§Ø±Ø³"}
    }

    try:
        if not os.path.exists('ranking_model_lite.tflite'): return None
        interpreter = Interpreter(model_path='ranking_model_lite.tflite')
        interpreter.allocate_tensors()

        scaler_X = joblib.load('scaler_X_lite.pkl')
        scaler_y = joblib.load('scaler_y_lite.pkl')
        
        indicator_names = []
        if os.path.exists('indicator_names_lite.txt'):
            with open('indicator_names_lite.txt', 'r', encoding='utf-8') as f:
                indicator_names = [line.strip() for line in f]
        
        feature_importance_map = joblib.load('feature_importance_map.pkl') if os.path.exists('feature_importance_map.pkl') else {}
        if not feature_importance_map and indicator_names:
            feature_importance_map = {name: 1.0 for name in indicator_names}

        return interpreter, scaler_X, scaler_y, indicator_names, recommendations_map, clusters, feature_importance_map
    
    except Exception as e:
        return None

loaded_assets = load_assets_lite()
if loaded_assets is None:
    st.error("âš ï¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…ÙÙ‚ÙˆØ¯Ø©. ØªØ£ÙƒØ¯ Ù…Ù† Ø±ÙØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (.tflite, .pkl).")
    st.stop()

interpreter, scaler_X, scaler_y, indicator_names, recommendations_map, clusters, feature_importance_map = loaded_assets

# ======================================================================
# -------------------- 3. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„Ù…Ø­Ø§ÙƒØ§Ø© (PARTS Core) --------------------
# ======================================================================

def forecast_future_values(df_history, target_year, indicators):
    """ Ø§Ù„ØªÙ†Ø¨Ø¤ (Prediction): Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙÙŠ PARTS """
    row_data = {}
    years_train = df_history['Ø§Ù„Ø³Ù†Ø©'].values.reshape(-1, 1)
    
    for col in indicators:
        if col in df_history.columns:
            model = LinearRegression()
            y_train = df_history[col].values
            model.fit(years_train, y_train)
            predicted_val = model.predict([[target_year]])[0]
            row_data[col] = max(0.0, min(100.0, predicted_val))
        else:
            # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 0ØŒ Ù†Ø¶Ø¹ Ù…ØªÙˆØ³Ø· (50) Ù„ØªØ¬Ù†Ø¨ Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            row_data[col] = 50.0 
    return row_data

def run_ai_model(input_values_dict, interpreter, scaler_X, scaler_y, indicator_names):
    """ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (AI-Driven) """
    values_list = [input_values_dict[name] for name in indicator_names]
    input_array = np.array([values_list]).astype(np.float32)
    
    # 1. Ø§Ù„ØªØ·Ø¨ÙŠØ¹
    X_scaled = scaler_X.transform(input_array)
    
    # 2. Ø§Ù„ØªÙ†Ø¨Ø¤
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.set_tensor(input_details[0]['index'], X_scaled)
    interpreter.invoke()
    y_scaled = interpreter.get_tensor(output_details[0]['index'])
    
    # 3. Ø¹ÙƒØ³ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
    rank = scaler_y.inverse_transform(y_scaled).flatten()[0]
    
    # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø³Ø§Ù„Ø¨Ø© (Ø§Ù„ØªØ±ØªÙŠØ¨ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø³Ø§Ù„Ø¨Ø§Ù‹)
    return max(1.0, rank)

def calculate_synergy(current_inputs, indicator_names, clusters):
    """ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¢Ø²Ø± (Synergy) """
    # Ù†Ø¹ØªØ¨Ø± Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¶Ø¹ÙŠÙØ© Ù‡ÙŠ Ø§Ù„ØªÙŠ ØªÙ‚Ù„ Ø¹Ù† 60%
    weak_inds = [name for name in indicator_names if current_inputs[name] < 60]
    
    hits = {c: len(set(weak_inds) & members) for c, members in clusters.items()}
    boost = 1.0 + (sum(1 for v in hits.values() if v >= 2) * 0.08)
    return min(boost, 1.25), weak_inds

# ======================================================================
# -------------------- 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (PARTS Framework UI) --------------------
# ======================================================================

st.set_page_config(layout="wide", page_title="Ù†Ø¸Ø§Ù… PARTS Ø§Ù„Ù‡Ø¬ÙŠÙ†")

st.markdown("""
    <style>
        .main { direction: rtl; }
        .stSlider > div { direction: rtl; }
        h1, h2, h3, p, div { text-align: right; font-family: 'Tahoma'; }
        .metric-card { background-color: #1e1e1e; padding: 15px; border-radius: 10px; border: 1px solid #333; text-align: center; }
        .highlight { color: #4CAF50; font-weight: bold; }
    </style>
""", unsafe_allow_html=True)

st.title("ğŸš€ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ù‡Ø¬ÙŠÙ† (Hybrid PARTS Model)")
st.markdown("---")

# --- 1. Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Input) ---
st.sidebar.header("ğŸ“‚ 1. Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©")
uploaded_file = st.sidebar.file_uploader("Ø§Ø±ÙØ¹ Ù…Ù„Ù Excel (Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©)", type=["xlsx"])

if uploaded_file is not None:
    df_history = pd.read_excel(uploaded_file)
    
    if 'Ø§Ù„Ø³Ù†Ø©' not in df_history.columns:
        st.error("ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ù…Ù„Ù Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ 'Ø§Ù„Ø³Ù†Ø©'")
        st.stop()
        
    last_year = int(df_history['Ø§Ù„Ø³Ù†Ø©'].max())
    target_year = st.sidebar.selectbox("Ø§Ø®ØªØ± Ø³Ù†Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©:", [last_year + i for i in range(1, 6)])
    
    # --- Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø£ÙˆÙ„ÙŠ (Forecast Baseline) ---
    forecasted_values = forecast_future_values(df_history, target_year, indicator_names)
    
    # --- 2. Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© ÙˆØ§Ù„ØªØ¢Ø²Ø± (Simulation & Synergy) ---
    st.header(f"ğŸ›ï¸ Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© Ù„Ø³Ù†Ø© {target_year}")
    st.info("ğŸ’¡ **Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø©:** Ø§Ù„Ù‚ÙŠÙ… Ø£Ø¯Ù†Ø§Ù‡ ØªÙ… Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù‡Ø§ Ø¢Ù„ÙŠØ§Ù‹ (AI Prediction). ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§ ÙŠØ¯ÙˆÙŠØ§Ù‹ (Simulation) Ù„Ø±Ø¤ÙŠØ© Ø£Ø«Ø± Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª.")
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø´Ø§Ø´Ø©
    col_sim, col_results = st.columns([1, 2])
    
    # >> Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© (Sliders)
    with col_sim:
        st.markdown("### ğŸ”§ Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª (Simulation)")
        user_inputs = {}
        for name in indicator_names:
            # Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù‡ÙŠ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…ØªÙ†Ø¨Ø£ Ø¨Ù‡Ø§
            default_val = float(forecasted_values[name])
            user_inputs[name] = st.slider(f"{name}", 0.0, 100.0, default_val, key=name)
            
            # Ø¹Ø±Ø¶ Ø§Ù„ÙØ±Ù‚ Ø¹Ù† Ø§Ù„ØªÙ†Ø¨Ø¤
            diff = user_inputs[name] - default_val
            if diff != 0:
                st.caption(f"ØªØºÙŠÙŠØ± Ø¹Ù† Ø§Ù„ØªÙ†Ø¨Ø¤: {diff:+.1f}%")

    # >> Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Results)
    with col_results:
        # 1. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ø§Ù„ÙŠØ© (Ø³ÙˆØ§Ø¡ ÙƒØ§Ù†Øª Ù…ØªÙ†Ø¨Ø£ Ø¨Ù‡Ø§ Ø£Ùˆ Ù…Ø¹Ø¯Ù„Ø©)
        current_rank = run_ai_model(user_inputs, interpreter, scaler_X, scaler_y, indicator_names)
        
        # 2. Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¢Ø²Ø±
        synergy_factor, weak_inds = calculate_synergy(user_inputs, indicator_names, clusters)
        
        st.markdown("### ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ù„ØªØ´Ø®ÙŠØµ (Analysis & Diagnosis)")
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù…ÙŠØªØ±ÙƒØ³
        m1, m2, m3 = st.columns(3)
        m1.metric("Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ (Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©)", f"{current_rank:.2f}")
        m2.metric("Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ¢Ø²Ø± (Synergy)", f"{synergy_factor:.2f}x")
        m3.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø±Ø¬Ø©", f"{len(weak_inds)}")
        
        # Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© (Baseline vs Simulation)
        st.markdown("#### ğŸ“ˆ Ø£Ø«Ø± Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨")
        
        # Ù†Ø­Ø³Ø¨ Ø§Ù„ØªØ±ØªÙŠØ¨ "Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ" (Ø¨Ø¯ÙˆÙ† ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…) Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        baseline_rank = run_ai_model(forecasted_values, interpreter, scaler_X, scaler_y, indicator_names)
        
        chart_data = pd.DataFrame({
            "Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ": ["Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø£ØµÙ„ÙŠ (Baseline)", "Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© (Simulation)"],
            "Ø§Ù„ØªØ±ØªÙŠØ¨ (Ø§Ù„Ø£Ù‚Ù„ Ø£ÙØ¶Ù„)": [baseline_rank, current_rank]
        })
        st.bar_chart(chart_data.set_index("Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ"), color=["#FF5722", "#4CAF50"])
        
        if current_rank < baseline_rank:
            st.success(f"âœ… Ù…Ø­Ø§ÙƒØ§ØªÙƒ Ø£Ø¯Øª Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¨Ù…Ù‚Ø¯Ø§Ø± {baseline_rank - current_rank:.2f} Ù†Ù‚Ø·Ø©!")
        elif current_rank > baseline_rank:
            st.warning(f"âš ï¸ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø£Ø¯Øª Ù„ØªØ±Ø§Ø¬Ø¹ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¨Ù…Ù‚Ø¯Ø§Ø± {current_rank - baseline_rank:.2f} Ù†Ù‚Ø·Ø©.")

        # Ø§Ù„ØªÙˆØµÙŠØ§Øª (Recommendations)
        st.markdown("### ğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠØ© (Recommendations)")
        if weak_inds:
            recs = []
            for ind in weak_inds:
                recs.append({
                    "Ø§Ù„Ù…Ø¤Ø´Ø±": ind,
                    "Ø§Ù„ØªÙˆØµÙŠØ©": recommendations_map.get(ind, "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø®Ø·Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ÙŠØ©"),
                    "Ø§Ù„Ø£Ù‡Ù…ÙŠØ©": f"{feature_importance_map.get(ind, 0.5):.2f}"
                })
            st.table(pd.DataFrame(recs))
        else:
            st.success("ğŸ‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ ÙˆØ¶Ø¹ Ù…Ù…ØªØ§Ø² ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø©!")

else:
    # Ø´Ø§Ø´Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨
    st.markdown("""
    <div style='text-align: center; padding: 50px;'>
        <h2>ğŸ‘‹ Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ ÙÙŠ Ù…Ù†ØµØ© PARTS Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø©</h2>
        <p>Ù„Ù„Ø¨Ø¯Ø¡ØŒ ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©.</p>
        <p style='color: gray;'>Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ø¸Ø§Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ø«Ù… ÙŠØªÙŠØ­ Ù„Ùƒ Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª.</p>
    </div>
    """, unsafe_allow_html=True)
